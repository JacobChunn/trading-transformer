{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobChunn/trading-transformer/blob/time_series_optimizations/trading-transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GpuopHgeCc1"
      },
      "source": [
        "# Trading Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    USING_COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Google Colab environment not detected, running locally.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqgZML3eZ1Xe",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgr-MnR4Z1Xf",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, bias):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        # Generate Scaled Gaussian Prior Bias & Time Splitter Mask (Ding et al., 2021) only at construction\n",
        "        self.gaussian_prior = self.generate_gaussian_prior(50, bias)\n",
        "        #self.timescale_mask = self.generate_timescale_mask(50, bias)\n",
        "        #torch.set_printoptions(profile=\"full\")\n",
        "        #print(\"timescale_mask: \", self.timescale_mask)\n",
        "        #torch.set_printoptions(profile=\"default\")\n",
        "\n",
        "    # Generate a mask with square sub matrices of a given side on the diagonal\n",
        "    def generate_timescale_mask(self, size, sub_size):\n",
        "        print(\"sub_size: \", sub_size)\n",
        "        t = torch.full((size, size), -1e4)\n",
        "        offset = 0\n",
        "        for i in range(size):\n",
        "            if (i+1) % (sub_size+1) == 0:\n",
        "                offset = offset + sub_size\n",
        "            for j in range(size):\n",
        "                if j < offset:\n",
        "                    continue\n",
        "                elif (j-offset) > (sub_size-1):\n",
        "                    break\n",
        "                else:\n",
        "                    t[i, j] = 0\n",
        "        return t\n",
        "    \n",
        "    def generate_gaussian_prior(self, size, scale):\n",
        "        g = torch.full((size, size), 0)\n",
        "        for i in range(size):\n",
        "            for j in range(size):\n",
        "                if j > i:\n",
        "                    break\n",
        "                else:\n",
        "                    g[i, j] = math.exp(-(math.pow((j - i), 2)/2*math.pow(scale, 2)))\n",
        "        return g\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        attn_scores = attn_scores + self.gaussian_prior # Apply Gaussian Prior\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e4) # changed from -1e9\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6pEcS-5Z1Xg",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU9esAQfZ1Xg",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrzUAm1nZ1Xh",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout, bias):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, bias)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrVcrDhyZ1Xh",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "class NextValuePredictor(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, bias):\n",
        "        super(NextValuePredictor, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout, bias*(_+1)) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, tgt):\n",
        "        #print(\"tgt\", tgt)\n",
        "        device = tgt.device\n",
        "        seq_length = tgt.size(1)\n",
        "        #print(\"seq_length\", seq_length)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2) & nopeak_mask\n",
        "        #print(\"tgt_mask: \", tgt_mask)\n",
        "        #print(\"tgt_mask.shape: \", tgt_mask.shape)\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, tgt):\n",
        "        tgt_mask = self.generate_mask(tgt)\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.embedding(tgt)))\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, tgt_mask)\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wQ5jx9-Z1Xh",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Read and preprocess the data\n",
        "if USING_COLAB:\n",
        "    data = pd.read_csv('/content/drive/Trading/Data/EURUSD-short.txt', skiprows=1, header=None)\n",
        "else:\n",
        "    data = pd.read_csv('EURUSD-short.txt', skiprows=1, header=None)\n",
        "data.columns = ['<TICKER>', '<DTYYYYMMDD>', '<TIME>', '<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<VOL>']\n",
        "data.columns = [col.strip('<>') for col in data.columns]\n",
        "\n",
        "data['TIME'] = data['TIME'].apply(lambda x: f'{int(x):06d}')\n",
        "data['DATETIME'] = pd.to_datetime(data['DTYYYYMMDD'].astype(str) + data['TIME'], format='%Y%m%d%H%M%S')\n",
        "\n",
        "# Extract relevant columns\n",
        "data = data[['DATETIME', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOL']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt4chd9_Z1Xh",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Create sequences using sliding window of window_size\n",
        "def create_full_sequences(data, window_size, predict_interval_count):\n",
        "    sequences = []\n",
        "    for i in range(0, len(data) - window_size, predict_interval_count):\n",
        "            sequences.append(data.iloc[i:i + window_size].values)\n",
        "    return np.array(sequences)\n",
        "\n",
        "predict_interval_count = 1 # Num of values to predict for each sequence\n",
        "window_size_indep = 50 # Num of intervals that spans across an input or target data val independently\n",
        "window_size = window_size_indep + predict_interval_count  # Num of intervals that spans across each input and target data val combined\n",
        "batch_size = 64    # Number of sequences per batch\n",
        "vocab_size = 20000 # number of classes (unique tokens)\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 512  # orig val is 2048\n",
        "max_seq_length = window_size  # Set to window_size - need to look into how this relates to window_size\n",
        "dropout = 0.1\n",
        "\n",
        "sequences = create_full_sequences(data[\"OPEN\"], window_size, predict_interval_count)\n",
        "#batch_sequences = get_batch(sequences, batch_size)\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "data_tensor = torch.tensor(sequences, dtype=torch.float32)\n",
        "\n",
        "data_tensor_int = (data_tensor * 10000).long()\n",
        "\n",
        "# Define the train-validation split\n",
        "train_input = data_tensor_int[:, :-1]\n",
        "train_target = data_tensor_int[:, 1:]\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(train_input, train_target)\n",
        "\n",
        "# Define the train-validation split\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy1FU3taZ1Xh",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Base Time-Scale Biasing (5-minutes)\n",
        "bias = 5\n",
        "# Model instance\n",
        "model = NextValuePredictor(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, bias).to(device)\n",
        "\n",
        "# Training\n",
        "#criterion = nn.MSELoss()\n",
        "criterion = nn.CrossEntropyLoss() #using cross entropy instead\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize GradScaler\n",
        "scaler = GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7VI3C_pZ1Xi",
        "metadata": {},
        "outputId": "4076fdb5-92a0-479d-a473-322de64c5f92"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(25):\n",
        "    # Training phase\n",
        "    for batch_input, batch_target in train_loader:\n",
        "        # Move data to the GPU\n",
        "        batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Autocast context manager\n",
        "        with autocast():\n",
        "            # Forward pass with mixed precision\n",
        "            output = model(batch_input)\n",
        "            # Compute loss with mixed precision\n",
        "            loss = criterion(output.contiguous().view(-1, vocab_size), batch_target.contiguous().view(-1))\n",
        "        # Backward pass with scaled loss to prevent underflow\n",
        "        scaler.scale(loss).backward()\n",
        "        # Update the parameters\n",
        "        scaler.step(optimizer)\n",
        "        # Update the scale for next iteration\n",
        "        scaler.update()\n",
        "        print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_input, batch_target in val_loader:\n",
        "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
        "            output = model(batch_input)\n",
        "            loss = criterion(output.view(-1, vocab_size), batch_target.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Average validation loss\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {loss.item()}, Validation Loss: {val_loss}\")\n",
        "    \n",
        "    # Set the model back to training mode\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkPTtAKTZ1Xi",
        "outputId": "4840c7c0-edef-43d9-819d-ae027eafc184"
      },
      "outputs": [],
      "source": [
        "# Final evaluation phase\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch_input, batch_target in test_loader:\n",
        "        batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
        "        output = model(batch_input)\n",
        "        loss = criterion(output.view(-1, vocab_size), batch_target.view(-1))\n",
        "        test_loss += loss.item()\n",
        "\n",
        "# Average test loss\n",
        "test_loss /= len(test_loader)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
